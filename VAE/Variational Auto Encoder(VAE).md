# Variational AutoEncoder

# Concept

> ë³€ë¶„í˜• ì˜¤í†  ì¸ì½”ë”ì˜ ëª©í‘œ

![img1](img/img1.png)

[Auto-Encoding Variational Bayes](https://jamiekang.github.io/2017/05/21/auto-encoding-variational-bayes/)ì™€ [ë”¥ëŸ¬ë‹ ê°œë… 1. VAE(Variational Auto Encoder)](https://velog.io/@ohado/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90-1.-VAEVariational-Auto-Encoder)
ë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.


> ì •ê·œë¶„í¬ëŠ” í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
> 

mean vector(í‰ê· ì˜ ì§‘í•©)ê³¼ standard deviation vector(í‘œì¤€í¸ì°¨ì˜ ì§‘í•©)ì„ í†µí•´ latent vector  z(`ì •ê·œë¶„í¬`)ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

ì‚¬ì§„ë“¤ì˜ ë¶„í¬ëŠ” ì¼ë°˜ì ì´ë¼ê³  ì—¬ê²¨ì§€ëŠ” ì •ê·œë¶„í¬ë¼ê³  ê°€ì •í•œë‹¤. `ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬`ì— ì˜í•´ì„œ ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜(from ìˆ˜ì§‘ëœ ìë£Œ)ëŠ” ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì§€ëŠ” ì„±ì§ˆì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

[*ìœ„í‚¤ ë°±ê³¼, ì •ê·œë¶„í¬*](https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%EB%B6%84%ED%8F%AC)

ê²°ì •ëœ ê³µê°„(í˜¹ì€ ê°€ì¤‘ì¹˜)ë¡œ í•™ìŠµí•˜ë©° ì„±ëŠ¥ì´ ì¢‹ì€ ë¶„í¬ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë‹¤.

| model | formula | distribution | role |
| --- | --- | --- | --- |
| unsupervised model | z= f(x) | p(z|x) | Encoder |
| generative model | x=g(z) | p(x|z) | Decoder |
- Encoderì˜ ì—­í• ì„ í•˜ëŠ” NNì€ Input data x ì—ì„œ ì ì¬ ë³€ìˆ˜ zë¥¼ ë§Œë“¤ê³ 
- Decoderì˜ ì—­í• ì„ í•˜ëŠ” NNì€ ì ì¬ë³€ìˆ˜ zì—ì„œ Input data xë¥¼ ë³µì›í•œë‹¤.

> VAEì˜ ê¸°ëŠ¥
> 

[[Steve-Lee's Deep Insight]](https://deepinsight.tistory.com/127#elbo-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0)

ì²« ì§¸, ì´ìƒì ì¸ ìƒ˜í”Œë§ í•¨ìˆ˜ë¡œ ë¶€í„° ìƒì„±í•œ zê°’ìœ¼ë¡œë¶€í„° (Training DBì— ìˆëŠ”) input dataì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ìƒì„±í•´ì¤˜Â â† Generattion

ë‘˜ ì§¸, ì´ìƒì ì¸ samplingí•¨ìˆ˜ì˜ ê°’ì´ ìµœëŒ€í•œ priorê°’ê³¼ ê°™ë„ë¡ ë§Œë“¤ì–´ì¤˜Â Â â† Condition

# [Encoder] Find Latent vector

ì ì¬ ê³µê°„ ë§Œë“¤ê¸°

## Maximum Likelihood Estimation

ë¶„í¬ë¥¼ ì°¾ê¸° ìœ„í•œ ì ‘ê·¼ë²•

[ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ ë…¸íŠ¸ì˜ <ìµœëŒ€ìš°ë„ë²•(MLE)>](https://angeloyeo.github.io/2020/07/17/MLE.html)ë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.

<aside>
ğŸ’¡ Key : ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì—ì„œ ë¶„í¬ë¥¼ ì¶”ì¸¡í•˜ê¸°

</aside>

- ì‚¬ê±´ : 1, ... , n
- ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ , likelyhood  : $P(n)$

![Untitled](img/Untitled%201.png)

![Untitled](img/Untitled%202.png)

> ê° ì‚¬ê±´ë“¤ì´ ë°œìƒí•  í™•ë¥ ì˜ ê³±ì„ ìµœëŒ€í™” í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ  ì ‘ê·¼
> 
- tip : ì •ê·œë¶„í¬ì—ì„œëŠ” ì‚¬ê±´ì€ ì¤‘ì•™ì— ë°€ì§‘í•´ ìˆë‹¤.

- ìì—°ë¡œê·¸($log_e$)ë¥¼ ì‚¬ìš©í•´ì„œ ê³±ì„ í•©ì˜ í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ ì»´í“¨í„°ì˜ ì—°ì‚°ì— ìœ ë¦¬í•œ í˜•íƒœë¡œ ë³€ê²½

![Untitled](img/Untitled%203.png)

![Untitled](img/Untitled%204.png)

## Variational Inference

[ratsgoâ€™s blogì˜ <ë³€ë¶„ì¶”ë¡ (Variational Inference)>](https://ratsgo.github.io/generative%20model/2017/12/19/vi/)ë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.

Variational Inference(ì´í•˜ VI)ëŠ” ì‚¬í›„í™•ë¥ (posterior) ë¶„í¬Â $P(z|x)$ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ í™•ë¥ ë¶„í¬(ì •ê·œë¶„í¬)Â $q(z)$ë¡œ ê·¼ì‚¬(approximation)í•˜ëŠ” ê²ƒì´ë‹¤.

> **KL divergenceë¥¼ ì´ìš©í•´ q(z)ë¡œ ê·¼ì‚¬í•œë‹¤.**
> 

[KL divergenceì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©](https://ratsgo.github.io/statistics/2017/09/22/information/)ì´ ìˆìŠµë‹ˆë‹¤.

ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler divergence, ì´í•˜ KLD)ê°€ ì¤„ì–´ë“œëŠ” ìª½ìœ¼ë¡œ $q(z)$ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ì‚¬í›„í™•ë¥ ì„ ì˜ ê·¼ì‚¬í•˜ëŠ” $q^*(z)$ë¥¼ ì–»ê²Œ ëœë‹¤.

MLE ë°©ì‹ìœ¼ë¡œ VAE ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì¶”ì •í•˜ê¸° 

- ë³€ë¶„ì¶”ë¡ ì„ í†µí•´ marginal log-likelihoodë¥¼ ìµœëŒ€í™”

![Untitled](img/Untitled%205.png)

![Untitled](img/Untitled%206.png)

ë…¸ì´ì¦ˆë¥¼ zero-mean Gausian(í‘œì¤€ì •ê·œë¶„í¬)ì—ì„œ í•˜ë‚˜ ë½‘ì•„ 2ê°œì˜ NN ($f_u, f_\sigma$)ê°€ ì‚°ì¶œí•œ í‰ê· ê³¼ ë¶„ì‚°ì„ ë”í•˜ê³  ê³±í•´ì¤˜ì„œ sampled latent vector zë¥¼ ë§Œë“ ë‹¤. *reparameterization trick ì´ë¼ê³  ë¶ˆë¦°ë‹¤.*

> zë¥¼ ì§ì ‘ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ë°©ì‹
> 

ì—­ì „íŒŒë¥¼ í†µí•´ í‰ê· ê³¼ ë¶„ì‚°ì„ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆê²Œ ë¨

ëœë¤ë¶„í¬ì—ì„œ zë¥¼ ìƒ˜í”Œë§ â†’ í•™ìŠµ(input xì— ì–´ë–¤ zë¥¼ ë„£ì–´ì•¼ í•˜ëŠ” ì§€ í•™ìŠµ)

*xì— ë”°ë¼ qì˜ ëª¨ìˆ˜(í‰ê· ,ë¶„ì‚°)ì´ ë°”ë€Œê²Œ ë˜ë¯€ë¡œ*

> Encoder ìš”ì•½
> 

<aside>
ğŸ’¡ EndoerëŠ” decoderì— ì–´ë–¤ ì…ë ¥ zë¥¼ ë„£ì„ ì§€ í•™ìŠµí•œë‹¤.

</aside>

![Untitled](img/Untitled%207.png)

![Untitled](img/Untitled%208.png)

> q(z) ì°¾ê¸° = ELBOë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ


ELBOì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…

[[Steve-Lee's Deep Insight]](https://deepinsight.tistory.com/127#elbo-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0)

![Untitled](img/Untitled%209.png)

![Untitled](img/Untitled%2010.png)

![Untitled](img/Untitled%2011.png)

![Untitled](img/Untitled%2012.png)

# [Decoder]

[Auto-Encoding Variational Bayes](https://jamiekang.github.io/2017/05/21/auto-encoding-variational-bayes/)ë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.

**DecoderëŠ” latent variable zë¡œ ë¶€í„° xë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” neural network**

## ë¬¸ì œ : 
`random` variable z â†’ `Sampling` â†’ `ë¯¸ë¶„ ë¶ˆê°€ëŠ¥` â†’ `Gradient X`

latent variableÂ zë¥¼ ë„£ìœ¼ë ¤ë©´Â zê°€ random variableì´ë¯€ë¡œ samplingì„ í•´ì•¼í•˜ëŠ”ë°, samplingì€ ë¯¸ë¶„ ê°€ëŠ¥í•˜ì§€ê°€ ì•Šì•„ì„œ gradientë¥¼ êµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤

## í•´ê²°ë°©ë²• : *reparameterization trick*

zì˜ stochasticí•œ ì„±ì§ˆì„ ë§ˆì¹˜ ìê¸° ìì‹ ì€ deterministicí•œë° ì™¸ë¶€ì—ì„œ random noiseÂ Ïµì´ ì…ë ¥ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë°”ê¿” ë²„ë¦½ë‹ˆë‹¤. ì¦‰ ì´ì œ VAEëŠ” parameterÂ Ï•(=Â $\mu_z(x), \sigma_z(x)$)ì— ëŒ€í•´ end-to-endë¡œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì´ ë©ë‹ˆë‹¤.

# ì¸¡ì • : regularizer

- p(z)ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ q(z|x)ë¥¼ ì‚¬ìš©í•˜ë©´ ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ê°€ ì†ì‹¤ ë˜ëŠ” ì§€ ì¸¡ì •
- q(z|x)ê°€ p(z)ì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ ì¸¡ì •í•˜ëŠ” measure

![Untitled](img/Untitled%2013.png)

MNIST ê°™ì€ image ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤ë©´, 

ë¶„í¬ ê°€ì •

- p(x|z) : Bernoulli
- q(z|x) : Gaussian

reconstruction lossë¥¼ ì…ì¶œë ¥ imageê°„ì˜ binary cross-entropyë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Binary cross-entropyë¥¼ ìˆ˜ì‹ëŒ€ë¡œ ê³„ì‚°

> ìˆ˜ì‹
> 

L(x)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì‰½ê²Œ ê³„ì‚°ë©ë‹ˆë‹¤. 

- ì£¼ì˜: gradient-descentë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë¶€í˜¸ê°€ ë°˜ëŒ€ë¡œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤.

```python
def vae_loss(y_true, y_pred):
    """ Calculate loss = reconstruction loss + KL loss for each data in minibatch """
    # E[log P(X|z)]
    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)
    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian
    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)

    return recon + kl
```

# Reference

[[ì •ë¦¬ë…¸íŠ¸] [AutoEncoderì˜ ëª¨ë“ ê²ƒ] Chap4. Variational AutoEncoderë€ ë¬´ì—‡ì¸ê°€(feat. ìì„¸íˆ ì•Œì•„ë³´ì)](https://deepinsight.tistory.com/127#elbo-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0)

[Auto-Encoding Variational Bayes](https://jamiekang.github.io/2017/05/21/auto-encoding-variational-bayes/)

[Variational AutoEncoder](https://ratsgo.github.io/generative%20model/2018/01/27/VAE/)